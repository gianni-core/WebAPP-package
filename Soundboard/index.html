<!DOCTYPE html>
<html lang="fr">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Soundboard en appel ‚Äì Demo</title>
  <style>
    body { font-family: system-ui, Arial, sans-serif; background:#0f172a; color:#e5e7eb; padding:20px }
    h1 { font-size: 1.4rem }
    button { padding:10px 14px; margin:6px; border-radius:10px; border:none; cursor:pointer }
    .primary { background:#22c55e; color:#052e16 }
    .secondary { background:#38bdf8; color:#042f2e }
    .danger { background:#f87171; color:#450a0a }
    .card { background:#020617; border-radius:16px; padding:16px; margin-top:16px; box-shadow:0 10px 30px rgba(0,0,0,.4)}
  </style>
</head>
<body>
  <h1>üéß Web App Soundboard en appel (WebRTC ‚Äì d√©mo)</h1>

  <div class="card">
    <p>1) Autorise le micro puis d√©marre l'appel (local loopback pour la d√©mo).</p>
    <button id="start" class="primary">D√©marrer l'appel</button>
    <button id="stop" class="danger">Stop</button>
  </div>

  <div class="card">
    <p>2) Joue un son qui sera inject√© dans l'appel :</p>
    <button class="secondary" data-sound="airhorn">üì£ Airhorn</button>
    <button class="secondary" data-sound="drum">ü•Å Drum</button>
  </div>

  <audio id="remote" autoplay></audio>

  <script>
    // --- Audio context & sounds ---
    const ctx = new AudioContext();
    const destination = ctx.createMediaStreamDestination();

    async function loadSound(url) {
      const res = await fetch(url);
      const buf = await res.arrayBuffer();
      return await ctx.decodeAudioData(buf);
    }

    const sounds = {};
    Promise.all([
      loadSound('https://actions.google.com/sounds/v1/alarms/air_horn.ogg').then(b => sounds.airhorn = b),
      loadSound('https://actions.google.com/sounds/v1/drums/medium_drum_roll.ogg').then(b => sounds.drum = b)
    ]);

    function playSound(name) {
      const src = ctx.createBufferSource();
      src.buffer = sounds[name];
      src.connect(destination); // injecte le son dans le flux d'appel
      src.start();
    }

    document.querySelectorAll('[data-sound]').forEach(btn => {
      btn.onclick = () => playSound(btn.dataset.sound);
    });

    // --- WebRTC (appel local loopback pour la d√©mo) ---
    let pc1, pc2, stream;

    document.getElementById('start').onclick = async () => {
      await ctx.resume();
      stream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // Mix micro + soundboard
      const micSource = ctx.createMediaStreamSource(stream);
      micSource.connect(destination);

      pc1 = new RTCPeerConnection();
      pc2 = new RTCPeerConnection();

      destination.stream.getTracks().forEach(track => pc1.addTrack(track, destination.stream));

      pc1.onicecandidate = e => e.candidate && pc2.addIceCandidate(e.candidate);
      pc2.onicecandidate = e => e.candidate && pc1.addIceCandidate(e.candidate);

      pc2.ontrack = e => document.getElementById('remote').srcObject = e.streams[0];

      const offer = await pc1.createOffer();
      await pc1.setLocalDescription(offer);
      await pc2.setRemoteDescription(offer);

      const answer = await pc2.createAnswer();
      await pc2.setLocalDescription(answer);
      await pc1.setRemoteDescription(answer);
    };

    document.getElementById('stop').onclick = () => {
      pc1?.close(); pc2?.close();
      stream?.getTracks().forEach(t => t.stop());
    };
  </script>
</body>
</html>
